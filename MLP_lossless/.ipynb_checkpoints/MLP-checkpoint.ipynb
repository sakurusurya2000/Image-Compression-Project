{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Image Compression Project MLP part\n",
    "\n",
    "This code applies predictive coding algoritm for lossless image compression with a basic MLP structure. Details of predictive coding algorithm can be found [here](https://web.stanford.edu/class/ee398a/handouts/lectures/06-Prediction.pdf)\n",
    "\n",
    "The code has four parts\n",
    "\n",
    "1. Huffman encoder (Coppied from [here](http://www.techrepublic.com/article/huffman-coding-in-python/))\n",
    "2. Creation of prediction blocks and label for predictive coding\n",
    "3. Linear regression algorithm for seeing the baseline\n",
    "4. MLP algorithm (initial phase)\n",
    "\n",
    "**PS:** I made all of the tests with a single image (lena512.bmp). JPEG-ls algorithm achieves 5.21 bpp (bits per pixel) with huffman coding, JPEG-2000-ls achieves ~4.31 bpp. MLP is able to achieve ~4.5 bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1: Huffman encoder and Entropy Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Binary tree data structure\n",
    "#http://www.techrepublic.com/article/huffman-coding-in-python/\n",
    "class Node(object):\n",
    "\tleft = None\n",
    "\tright = None\n",
    "\titem = None\n",
    "\tweight = 0\n",
    "\n",
    "\tdef __init__(self, i, w):\n",
    "\t\tself.item = i\n",
    "\t\tself.weight = w\n",
    "\n",
    "\tdef setChildren(self, ln, rn):\n",
    "\t\tself.left = ln\n",
    "\t\tself.right = rn\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"%s - %s â€” %s _ %s\" % (self.item, self.weight, self.left, self.right)\n",
    "\n",
    "\tdef __cmp__(self, a):\n",
    "\t\treturn cmp(self.weight, a.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Huffman Encoder\n",
    "#http://www.techrepublic.com/article/huffman-coding-in-python/\n",
    "\n",
    "from itertools import groupby\n",
    "from heapq import *\n",
    "\n",
    "\n",
    "#Huffman encoder  \n",
    "def huffman(input):\n",
    "    itemqueue =  [Node(a,len(list(b))) for a,b in groupby(sorted(input))]\n",
    "    heapify(itemqueue)\n",
    "    while len(itemqueue) > 1:\n",
    "        l = heappop(itemqueue)\n",
    "        r = heappop(itemqueue)\n",
    "        n = Node(None, r.weight+l.weight)\n",
    "        n.setChildren(l,r)\n",
    "        heappush(itemqueue, n) \n",
    "        \n",
    "    codes = {}\n",
    "    def codeIt(s, node):\n",
    "        if node.item:\n",
    "            if not s:\n",
    "                codes[node.item] = \"0\"\n",
    "            else:\n",
    "                codes[node.item] = s\n",
    "        else:\n",
    "            codeIt(s+\"0\", node.left)\n",
    "            codeIt(s+\"1\", node.right)\n",
    "    codeIt(\"\",itemqueue[0])\n",
    "    return codes, \"\".join([codes[a] for a in input])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitrate of the original image\n",
      "Bits per pixel is 7.46820831299 bpp\n"
     ]
    }
   ],
   "source": [
    "#Test Huffman encoder with an image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "img=mpimg.imread('lena512.bmp')\n",
    "\n",
    "img_input=img.reshape([-1]).astype(str)\n",
    "huffman_img = huffman(img_input)\n",
    "\n",
    "print('Bitrate of the original image')\n",
    "print('Bits per pixel is ' + str(float(len(huffman_img[1])/float(len(img_input)))) + ' bpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def entropy(labels,degree):\n",
    "    \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "\n",
    "    [counts, bins]  = np.histogram(labels,np.append(np.unique(labels),np.inf))\n",
    "    probs = counts.astype(float) / float(n_labels)\n",
    "    \n",
    "\n",
    "    # Compute standard entropy.\n",
    "    if(degree==1):\n",
    "        ent = -np.sum(np.multiply(probs,np.log2(probs)))\n",
    "    else:\n",
    "        ent = np.log2(np.sum(np.power(probs,degree)))/(1-degree)\n",
    "\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2: Creation of prediction blocks and label for predictive coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lossless image copmpression using predictive coding. For reference see below\n",
    "#(https://web.stanford.edu/class/ee398a/handouts/lectures/06-Prediction.pdf)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "#Returns prediction blocks and the corresponding pixels in the image\n",
    "#Very naive implementation, neglects boundaries, can be improved further\n",
    "def pred_vectors(img,pred_size):\n",
    "    (n,m)=img.shape #image size\n",
    "    k,l=pred_size #Size of the predictive window\n",
    "    \n",
    "    fvec=np.zeros([(n-k-1)*(m-2*l),2*k*l+k+l])\n",
    "    #print(fvec.shape)\n",
    "    label = np.zeros([(n-k-1)*(m-2*l),1])\n",
    "    for (i,j) in product(range(k,n-1), range(l,m-l)):\n",
    "        #print(i,j)\n",
    "        idx = (i-k)*(m-2*l)+j-l\n",
    "        fvec_current =img[i-k:i,j-l:j+l+1].reshape([-1])\n",
    "        fvec_current = np.append(fvec_current,img[i,j-l:j].reshape([-1]))\n",
    "        fvec[idx,:]=fvec_current\n",
    "        label[idx]=img[i,j]\n",
    "        \n",
    "    return fvec, label\n",
    "\n",
    "\n",
    "\n",
    "fvec,label = pred_vectors(img,[5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-3: Linear regression algorithm for seeing the baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with linear regression\n",
      "MSE is 33.3905996567\n",
      "Bits per pixel is 4.43457789396 bpp\n"
     ]
    }
   ],
   "source": [
    "#First trial: Simple regression network. No relation to deep learning just to gain some intuition\n",
    "\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "\n",
    "#Create the regression model using sklearn\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(fvec, label)\n",
    "\n",
    "#Predict and quantize the labels\n",
    "label_pred = np.round(regr.predict(fvec))\n",
    "\n",
    "#Calculate the error\n",
    "err=label_pred-label;\n",
    "\n",
    "print('Results with linear regression')\n",
    "#MSE\n",
    "print('MSE is '  + str(np.mean(err**2)))\n",
    "\n",
    "#Calculate Huffman coding of the error\n",
    "huffman_err = huffman(err.reshape([-1]).astype(str))\n",
    "print('Bits per pixel is ' + str(float(len(huffman_err[1])/float(len(err)))) + ' bpp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4: MLP algorithm (initial phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second trial: MLP\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def mlp(x, hidden_sizes, activation_fn=tf.nn.relu,dropout_rate=1.0,std_dev=1.0):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=std_dev)}\n",
    "    for k in range(len(hidden_sizes)-1):\n",
    "        layer_name=\"weights\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[k]])\n",
    "            #b = tf.get_variable('b', shape=[hidden_sizes[k]])\n",
    "            x = activation_fn(tf.matmul(x, W))# + b)\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[-1]])\n",
    "        #b = tf.get_variable('b', shape=[hidden_sizes[-1]])\n",
    "        return tf.matmul(x, W)# + b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4-a: MLP with MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\t mse loss: 0.50684\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 2000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 4000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 6000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 8000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 10000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 12000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 14000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 16000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 18000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 20000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 22000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 24000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 26000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 28000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n",
      "iteration 30000\t mse loss: 0.50683\t entropy1: 7.44737\t entropy2: 7.32379\t Huffman bitrate is 7.470\n"
     ]
    }
   ],
   "source": [
    "##WORKING CODE WITH MSE\n",
    "#Normalize the vectors and labels\n",
    "#Sometimes does not work beacuse of bad initialization\n",
    "#If MSE does not change in the first oteration rerun the code.\n",
    "#Note that in the compresison algorithm we can run it as many times\n",
    "#as we want and chose the best one.\n",
    "\n",
    "fvec_n=fvec/np.round(np.max(label))\n",
    "label_n = label/np.round(np.max(label))\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, fvec_n.shape[1]])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "\n",
    "            loss = tf.reduce_mean(tf.abs(tf.subtract(y_,y_logits)))\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=5e-3,beta1=0.3,beta2=0.999, \n",
    "                                                epsilon=1e-08,use_locking=False).minimize(loss)\n",
    "           \n",
    "            y_pred = y_logits\n",
    "            correct_prediction = tf.equal(y_pred, y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        ids=[i for i in range(100)]\n",
    "        for iter_i in range(30001):\n",
    "            batch_xs = fvec_n[ids,:] \n",
    "            batch_ys = label_n[ids]\n",
    "            ids=[(ids[0]+100+i)%label.shape[0] for i in range(100)]\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 2000 == 0:\n",
    "                tf_feed_dict = {x_: fvec_n, y_: label_n}\n",
    "                acc_value = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "                #print(err_value.reshape([-1]))\n",
    "                entropy1_err = entropy(err_value.reshape([-1]),1)\n",
    "                entropy2_err = entropy(err_value.reshape([-1]),2)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print('iteration %d\\t mse loss: %.5f\\t entropy1: %.5f\\t entropy2: %.5f\\t Huffman bitrate is %.3f'%\n",
    "                      (iter_i, acc_value, entropy1_err,entropy2_err,huffman_bpp))\n",
    "        err_value =  np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "        #print(err_value)\n",
    "                \n",
    "test_classification(lambda x: mlp(x, [32,16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4-b: MLP with Entropy Loss\n",
    "\n",
    "**Lookup:** Parzen Windowing for entropy estimation\n",
    "\n",
    "Renyi's Entropy with degree 2\n",
    "\n",
    "This part is not completed and not included in the report. We are planning work on it in the summer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Test for Renyi's Entropy\n",
    "\n",
    "from pprint import pprint\n",
    "#x=np.array([[1.,1.,1.,1.,1.,1.,1.,1.,2.,2.,2.,2.,2.,3.,3.,3.,4.,4.,5.,5.]])\n",
    "n=9\n",
    "x=np.array([[1.2,2.7,3.1,4.65,5.32,6.123,7.21312,8.8,9.12]])#,\n",
    "#            [1,3,28,10,23,12,43,1,9],[-4,0,2,1,54,1,3,8,-4]])\n",
    "x=x.astype(np.float32)\n",
    "\n",
    "c=np.power(n,-.2)\n",
    "print(x.shape)\n",
    "err_ = tf.placeholder(tf.float32, [None, n])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "#x_std=tf.reduce_mean(x_)\n",
    "x_mean,x_std=tf.nn.moments(err_,axes=[1])\n",
    "x_std=tf.sqrt(tf.multiply(x_std,float(n)/float(n-1)))\n",
    "h_=tf.reshape(tf.multiply(1.06,tf.multiply(x_std,c)),[-1,1])\n",
    "\n",
    "\n",
    "\n",
    "#prob_mat1=tf.div(err_,tf.matmul(h_,np.ones([1,n]).astype(np.float32)))\n",
    "#Calculate Entropy\n",
    "prob_mat = tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_+50),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "#Calculate Entropy\n",
    "sum_for_ent=tf.pow(prob_,2)\n",
    "\n",
    "\n",
    "for k in range(1,100):\n",
    "    #print(k)\n",
    "    prob_mat = tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_-k+50),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "    prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "    #Calculate Entropy\n",
    "    sum_for_ent=sum_for_ent+tf.pow(prob_,2)\n",
    "renyi_ent2=-tf.log(sum_for_ent)#/tf.log(2.)\n",
    "loss=tf.reduce_mean(renyi_ent2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    f_val=sess.run(renyi_ent2, feed_dict={err_:x})\n",
    "    f_val2=sess.run(loss, feed_dict={err_:x})\n",
    "    pprint(f_val)\n",
    "    pprint(f_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#It was wotking before but know stucks at zero, dont know the reason\n",
    "\n",
    "#Normalize the vectors and labels\n",
    "#Sometimes does not work beacuse of wron initialization\n",
    "\n",
    "fvec_n=fvec/np.round(np.max(label))\n",
    "label_n = label/np.round(np.max(label))\n",
    "\n",
    "\n",
    "def test_classification(model_function, learning_rate=0.1,batch_size=128,batch_overlap=.7):\n",
    "\n",
    "    n=batch_size\n",
    "    batch_inc = int(batch_size*(1-batch_overlap))\n",
    "    print('Batch size is %d, batch overalpping is %.2f'%(batch_size,batch_overlap))\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, fvec_n.shape[1]])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            #Basic MSE loss\n",
    "            #loss = tf.reduce_mean(tf.pow(tf.subtract(y_,y_logits), 2.0))\n",
    "            \n",
    "            #RENYI ENTROPY IS NOT DIFFERENTIABLE\n",
    "            #kernel = tf.contrib.distributions.Normal(mu=0., sigma=1.)\n",
    "            #loss = tf.contrib.bayesflow.entropy.entropy_shannon(p=kernel,z=tf.subtract(y_,y_logits))\n",
    "            #loss = tf.reduce_mean(tf.abs(tf.subtract(y_,y_logits)))\n",
    "            #\"\"\"\n",
    "            \n",
    "            c=np.power(n,-.2).astype(np.float32)\n",
    "            err_=tf.transpose(tf.subtract(y_,y_logits))*255\n",
    "            x_mean,x_std=tf.nn.moments(err_,axes=[1])\n",
    "            x_std=tf.sqrt(tf.multiply(x_std,float(n)/float(n-1)))\n",
    "            h_=tf.reshape(tf.multiply(1.06,tf.multiply(x_std,c)),[-1,1])\n",
    "            #h_=tf.multiply(1.06,tf.multiply(x_std,c))\n",
    "\n",
    "\n",
    "            #prob_mat1=tf.div(err_,tf.matmul(h_,np.ones([1,n]).astype(np.float32)))\n",
    "            #Calculate Entropy\n",
    "            prob_mat = 1e-3+tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_+255),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "            prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "            #Calculate Entropy\n",
    "            sum_for_ent=tf.pow(prob_,2)\n",
    "\n",
    "\n",
    "            for k in range(1,510):\n",
    "                #print(k)\n",
    "                prob_mat = 1e-3+tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_-k+255),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "                prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "                #Calculate Entropy\n",
    "                sum_for_ent=sum_for_ent+tf.pow(prob_,2)\n",
    "            renyi_ent2=-tf.log(sum_for_ent)#/tf.log(2.)\n",
    "            loss=tf.reduce_mean(renyi_ent2)\n",
    "            #\"\"\"\n",
    "            \n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "            #train_step = tf.train.AdamOptimizer(learning_rate=5e-3,beta1=0.3,beta2=0.999, \n",
    "            #                                    epsilon=1e-08,use_locking=False).minimize(loss)\n",
    "           \n",
    "            y_pred = y_logits\n",
    "            correct_prediction = tf.equal(y_pred, y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        #print(label.shape[0])\n",
    "        \n",
    "        ids=[i for i in range(batch_size)]\n",
    "        for iter_i in range(100001):\n",
    "            #print(iter_i)\n",
    "            #print(label.shape[0])\n",
    "            #print(2*my_range)\n",
    "            batch_xs = fvec_n[ids,:] \n",
    "            batch_ys = label_n[ids]\n",
    "            #print(ids)\n",
    "            ids=[(ids[0]+batch_inc+i)%label.shape[0] for i in range(batch_size)]\n",
    "            \n",
    "            tf_feed_dict = {x_: batch_xs, y_: batch_ys}\n",
    "            #acc_value = sess.run(c, feed_dict=tf_feed_dict)\n",
    "            '''\n",
    "            print(c)\n",
    "            acc_value = sess.run(err_, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value = sess.run(x_std, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value = sess.run(h_, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value2 = sess.run(y_logits, feed_dict=tf_feed_dict)\n",
    "            acc_value3 = sess.run(prob_mat, feed_dict=tf_feed_dict)\n",
    "            y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "            err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys)*255)\n",
    "            \n",
    "            print(acc_value2)\n",
    "            print(acc_value3)\n",
    "            '''\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 100 == 0:\n",
    "                #tf_feed_dict = {x_: fvec_n, y_: label_n}\n",
    "                tf_feed_dict = {x_: batch_xs, y_: batch_ys}\n",
    "                acc_value = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys)*255)\n",
    "                \n",
    "                '''\n",
    "                acc_value2 = sess.run(y_logits, feed_dict=tf_feed_dict)\n",
    "                acc_value3 = sess.run(prob_mat, feed_dict=tf_feed_dict)\n",
    "                print(acc_value)\n",
    "                print(acc_value2)\n",
    "                print(acc_value3)\n",
    "                print(err_value.reshape([-1]))\n",
    "                '''\n",
    "                entropy1_err = entropy(err_value.reshape([-1]),1)\n",
    "                entropy2_err = entropy(err_value.reshape([-1]),2)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print(y_pred_val*255)\n",
    "                print('iteration %d\\t mse loss: %.5f\\t entropy1: %.5f\\t entropy2: %.5f\\t Huffman bitrate is %.3f'%\n",
    "                      (iter_i, acc_value, entropy1_err,entropy2_err,huffman_bpp))\n",
    "                err_value =  np.round((sess.run(y_pred, feed_dict={x_: fvec_n, y_: label_n})-label_n)*255)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print('Huffman bitrate is %.3f'%huffman_bpp)\n",
    "                \n",
    "test_classification(lambda x: mlp(x, [16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_classification(lambda x: mlp(x, [32,16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learn_rate=5e-3\n",
    "                    ,batch_size=128,batch_overlap=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TEST PHASE. MAY NOT WORK\n",
    "#Look for modularity. You can use the old homework\n",
    "\n",
    "fvec_n=fvec/np.round(np.max(label))\n",
    "label_n = label/np.round(np.max(label))\n",
    "def build_entropy_network(model_function, learning_rate=0.1,batch_size=128,batch_overlap=.7):\n",
    "\n",
    "    n=batch_size\n",
    "    batch_inc = int(batch_size*(1-batch_overlap))\n",
    "    print('Batch size is %d, batch overalpping is %.2f'%(batch_size,batch_overlap))\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, fvec_n.shape[1]])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            #Basic MSE loss\n",
    "            #loss = tf.reduce_mean(tf.pow(tf.subtract(y_,y_logits), 2.0))\n",
    "            \n",
    "            #RENYI ENTROPY IS NOT DIFFERENTIABLE\n",
    "            #kernel = tf.contrib.distributions.Normal(mu=0., sigma=1.)\n",
    "            #loss = tf.contrib.bayesflow.entropy.entropy_shannon(p=kernel,z=tf.subtract(y_,y_logits))\n",
    "            #loss = tf.reduce_mean(tf.abs(tf.subtract(y_,y_logits)))\n",
    "            #\"\"\"\n",
    "            \n",
    "            c=np.power(n,-.2).astype(np.float32)\n",
    "            err_=tf.transpose(tf.subtract(y_,y_logits))*255\n",
    "            x_mean,x_std=tf.nn.moments(err_,axes=[1])\n",
    "            x_std=tf.sqrt(tf.multiply(x_std,float(n)/float(n-1)))\n",
    "            h_=tf.reshape(tf.multiply(1.06,tf.multiply(x_std,c)),[-1,1])\n",
    "            #h_=tf.multiply(1.06,tf.multiply(x_std,c))\n",
    "\n",
    "\n",
    "            #prob_mat1=tf.div(err_,tf.matmul(h_,np.ones([1,n]).astype(np.float32)))\n",
    "            #Calculate Entropy\n",
    "            prob_mat = 1e-3+tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_+255),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "            prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "            #Calculate Entropy\n",
    "            sum_for_ent=tf.pow(prob_,2)\n",
    "\n",
    "\n",
    "            for k in range(1,510):\n",
    "                #print(k)\n",
    "                prob_mat = 1e-3+tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_-k+255),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "                prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "                #Calculate Entropy\n",
    "                sum_for_ent=sum_for_ent+tf.pow(prob_,2)\n",
    "            renyi_ent2=-tf.log(sum_for_ent)#/tf.log(2.)\n",
    "            loss=tf.reduce_mean(renyi_ent2)\n",
    "            #\"\"\"\n",
    "            \n",
    "            #train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=5e-3,beta1=0.3,beta2=0.999, \n",
    "                                                epsilon=1e-08,use_locking=False).minimize(loss)\n",
    "           \n",
    "            y_pred = y_logits\n",
    "            correct_prediction = tf.equal(y_pred, y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return g\n",
    "\n",
    "            \n",
    "def test_classification(model_function, g,batch_size=128,batch_overlap=.7):  \n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        #print(label.shape[0])\n",
    "        \n",
    "        ids=[i for i in range(batch_size)]\n",
    "        for iter_i in range(100001):\n",
    "            #print(iter_i)\n",
    "            #print(label.shape[0])\n",
    "            #print(2*my_range)\n",
    "            batch_xs = fvec_n[ids,:] \n",
    "            batch_ys = label_n[ids]\n",
    "            #print(ids)\n",
    "            ids=[(ids[0]+batch_inc+i)%label.shape[0] for i in range(batch_size)]\n",
    "            \n",
    "            tf_feed_dict = {x_: batch_xs, y_: batch_ys}\n",
    "            #acc_value = sess.run(c, feed_dict=tf_feed_dict)\n",
    "            '''\n",
    "            print(c)\n",
    "            acc_value = sess.run(err_, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value = sess.run(x_std, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value = sess.run(h_, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value2 = sess.run(y_logits, feed_dict=tf_feed_dict)\n",
    "            acc_value3 = sess.run(prob_mat, feed_dict=tf_feed_dict)\n",
    "            y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "            err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys)*255)\n",
    "            \n",
    "            print(acc_value2)\n",
    "            print(acc_value3)\n",
    "            '''\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 100 == 0:\n",
    "                #tf_feed_dict = {x_: fvec_n, y_: label_n}\n",
    "                tf_feed_dict = {x_: batch_xs, y_: batch_ys}\n",
    "                acc_value = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys)*255)\n",
    "                \n",
    "                '''\n",
    "                acc_value2 = sess.run(y_logits, feed_dict=tf_feed_dict)\n",
    "                acc_value3 = sess.run(prob_mat, feed_dict=tf_feed_dict)\n",
    "                print(acc_value)\n",
    "                print(acc_value2)\n",
    "                print(acc_value3)\n",
    "                print(err_value.reshape([-1]))\n",
    "                '''\n",
    "                entropy1_err = entropy(err_value.reshape([-1]),1)\n",
    "                entropy2_err = entropy(err_value.reshape([-1]),2)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                #print(y_pred_val*255)\n",
    "                print('iteration %d\\t mse loss: %.5f\\t entropy1: %.5f\\t entropy2: %.5f\\t Huffman bitrate is %.3f'%\n",
    "                      (iter_i, acc_value, entropy1_err,entropy2_err,huffman_bpp))\n",
    "                err_value =  np.round((sess.run(y_pred, feed_dict={x_: fvec_n, y_: label_n})-label_n)*255)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print('Huffman bitrate is %.3f'%huffman_bpp)\n",
    "                \n",
    "g=build_entropy_network(lambda x: mlp(x, [32,16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=0.01\n",
    "                    ,batch_size=128,batch_overlap=.7)\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
