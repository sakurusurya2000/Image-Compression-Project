{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Image Compression Project MLP part\n",
    "\n",
    "This code applies predictive coding algoritm with a basic MLP structure. Details of predictive coding algorithm can be found [here](https://web.stanford.edu/class/ee398a/handouts/lectures/06-Prediction.pdf)\n",
    "\n",
    "The code has four parts\n",
    "\n",
    "1. Huffman encoder (Coppied from [here](http://www.techrepublic.com/article/huffman-coding-in-python/))\n",
    "2. Creation of prediction blocks and label for predictive coding\n",
    "3. Linear regression algorithm for seeing the baseline\n",
    "4. MLP algorithm (initial phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1: Huffman encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Binary tree data structure\n",
    "#http://www.techrepublic.com/article/huffman-coding-in-python/\n",
    "class Node(object):\n",
    "\tleft = None\n",
    "\tright = None\n",
    "\titem = None\n",
    "\tweight = 0\n",
    "\n",
    "\tdef __init__(self, i, w):\n",
    "\t\tself.item = i\n",
    "\t\tself.weight = w\n",
    "\n",
    "\tdef setChildren(self, ln, rn):\n",
    "\t\tself.left = ln\n",
    "\t\tself.right = rn\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"%s - %s â€” %s _ %s\" % (self.item, self.weight, self.left, self.right)\n",
    "\n",
    "\tdef __cmp__(self, a):\n",
    "\t\treturn cmp(self.weight, a.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Huffman Encoder\n",
    "#http://www.techrepublic.com/article/huffman-coding-in-python/\n",
    "\n",
    "from itertools import groupby\n",
    "from heapq import *\n",
    "\n",
    "\n",
    "#Huffman encoder  \n",
    "def huffman(input):\n",
    "    itemqueue =  [Node(a,len(list(b))) for a,b in groupby(sorted(input))]\n",
    "    heapify(itemqueue)\n",
    "    while len(itemqueue) > 1:\n",
    "        l = heappop(itemqueue)\n",
    "        r = heappop(itemqueue)\n",
    "        n = Node(None, r.weight+l.weight)\n",
    "        n.setChildren(l,r)\n",
    "        heappush(itemqueue, n) \n",
    "        \n",
    "    codes = {}\n",
    "    def codeIt(s, node):\n",
    "        if node.item:\n",
    "            if not s:\n",
    "                codes[node.item] = \"0\"\n",
    "            else:\n",
    "                codes[node.item] = s\n",
    "        else:\n",
    "            codeIt(s+\"0\", node.left)\n",
    "            codeIt(s+\"1\", node.right)\n",
    "    codeIt(\"\",itemqueue[0])\n",
    "    return codes, \"\".join([codes[a] for a in input])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitrate of the original image\n",
      "Bits per pixel is 7.46820831299 bpp\n"
     ]
    }
   ],
   "source": [
    "#Test Huffman encoder with an image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "img=mpimg.imread('lena512.bmp')\n",
    "#print(img.shape)\n",
    "#imgplot=plt.imshow(img,cmap='gray')\n",
    "\n",
    "img_input=img.reshape([-1]).astype(str)\n",
    "#print(img_input)\n",
    "huffman_img = huffman(img_input)\n",
    "#print(huffman_img[1])\n",
    "\n",
    "#print('Huffman code for ' + str(img) + ' is ' + str(huffman_img))\n",
    "#print('Original length is '+str(len(input) * 8)+', length of huffman coding is '+ str(len(huffman(input)[1])))\n",
    "print('Bitrate of the original image')\n",
    "print('Bits per pixel is ' + str(float(len(huffman_img[1])/float(len(img_input)))) + ' bpp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2: Creation of prediction blocks and label for predictive coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lossless image copmpression using predictive coding. For reference see below\n",
    "#(https://web.stanford.edu/class/ee398a/handouts/lectures/06-Prediction.pdf)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "#Returns prediction blocks and the corresponding pixels in the image\n",
    "#Very naive implementation, neglects boundaries, can be improved further\n",
    "def pred_vectors(img,pred_size):\n",
    "    (n,m)=img.shape #image size\n",
    "    k,l=pred_size #Size of the predictive window\n",
    "    \n",
    "    fvec=np.zeros([(n-k-1)*(m-2*l),2*k*l+k+l])\n",
    "    #print(fvec.shape)\n",
    "    label = np.zeros([(n-k-1)*(m-2*l),1])\n",
    "    for (i,j) in product(range(k,n-1), range(l,m-l)):\n",
    "        #print(i,j)\n",
    "        idx = (i-k)*(m-2*l)+j-l\n",
    "        fvec_current =img[i-k:i,j-l:j+l+1].reshape([-1])\n",
    "        fvec_current = np.append(fvec_current,img[i,j-l:j].reshape([-1]))\n",
    "        fvec[idx,:]=fvec_current\n",
    "        label[idx]=img[i,j]\n",
    "        \n",
    "    return fvec, label\n",
    "\n",
    "\n",
    "\n",
    "fvec,label = pred_vectors(img,[3,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-3: Linear regression algorithm for seeing the baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with linear regression\n",
      "MSE is 33.4797180849\n",
      "Bits per pixel is 4.43469547481 bpp\n"
     ]
    }
   ],
   "source": [
    "#First trial: Simple regression network. No relation to deep learning just to gain some intuition\n",
    "\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "\n",
    "#Create the regression model using sklearn\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(fvec, label)\n",
    "\n",
    "#Predict and quantize the labels\n",
    "label_pred = np.round(regr.predict(fvec))\n",
    "\n",
    "#Calculate the error\n",
    "err=label_pred-label;\n",
    "\n",
    "print('Results with linear regression')\n",
    "#MSE\n",
    "print('MSE is '  + str(np.mean(err**2)))\n",
    "\n",
    "#Calculate Huffman coding of the error\n",
    "huffman_err = huffman(err.reshape([-1]).astype(str))\n",
    "print('Bits per pixel is ' + str(float(len(huffman_err[1])/float(len(err)))) + ' bpp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4: MLP algorithm (initial phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second trial: MLP\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def mlp(x, hidden_sizes, activation_fn=tf.nn.relu,dropout_rate=1.0,std_dev=1.0):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=std_dev)}\n",
    "    for k in range(len(hidden_sizes)-1):\n",
    "        layer_name=\"weights\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[k]])\n",
    "            #b = tf.get_variable('b', shape=[hidden_sizes[k]])\n",
    "            x = activation_fn(tf.matmul(x, W))# + b)\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[-1]])\n",
    "        #b = tf.get_variable('b', shape=[hidden_sizes[-1]])\n",
    "        return tf.matmul(x, W)# + b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-f40665fb6147>:42: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "iteration 0\t mse loss: 0.29553\t Huffman bitrate is 7.474\n",
      "iteration 2000\t mse loss: 0.00106\t Huffman bitrate is 4.802\n",
      "iteration 4000\t mse loss: 0.00101\t Huffman bitrate is 4.725\n",
      "iteration 6000\t mse loss: 0.00085\t Huffman bitrate is 4.723\n",
      "iteration 8000\t mse loss: 0.00109\t Huffman bitrate is 4.704\n",
      "iteration 10000\t mse loss: 0.00072\t Huffman bitrate is 4.619\n",
      "iteration 12000\t mse loss: 0.00069\t Huffman bitrate is 4.620\n"
     ]
    }
   ],
   "source": [
    "#Normalize the vectors and labels\n",
    "#Sometimes does not work beacuse of wron initialization\n",
    "\n",
    "fvec_n=fvec/np.round(np.max(label))\n",
    "label_n = label/np.round(np.max(label))\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, fvec_n.shape[1]])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            #Basic MSE loss\n",
    "            loss = tf.reduce_mean(tf.pow(tf.subtract(y_,y_logits), 2.0))\n",
    "            #loss = tf.reduce_mean(tf.abs(tf.subtract(y_,y_logits)))\n",
    "            #train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=5e-3,beta1=0.3,beta2=0.999, \n",
    "                                                epsilon=1e-08,use_locking=False).minimize(loss)\n",
    "           \n",
    "            y_pred = y_logits\n",
    "            correct_prediction = tf.equal(y_pred, y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        #print(label.shape[0])\n",
    "        ids=[i for i in range(100)]\n",
    "        for iter_i in range(50001):\n",
    "            #print(label.shape[0])\n",
    "            #print(2*my_range)\n",
    "            batch_xs = fvec_n[ids,:] \n",
    "            batch_ys = label_n[ids]\n",
    "            ids=[(ids[0]+100+i)%label.shape[0] for i in range(100)]\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 2000 == 0:\n",
    "                tf_feed_dict = {x_: fvec_n, y_: label_n}\n",
    "                acc_value = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print('iteration %d\\t mse loss: %.5f\\t Huffman bitrate is %.3f'%(iter_i, acc_value, huffman_bpp))\n",
    "        err_value =  np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "        print(err_value)\n",
    "                \n",
    "test_classification(lambda x: mlp(x, [32,16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
