{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import read_cifar10 as cf10\n",
    "\n",
    "#@read_data.restartable\n",
    "def cifar10_dataset_generator(dataset_name, batch_size, restrict_size=1000):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    X_all_unrestricted, y_all = (cf10.load_training_data() if dataset_name == 'train'\n",
    "                                 else cf10.load_test_data())\n",
    "    \n",
    "    actual_restrict_size = restrict_size if dataset_name == 'train' else int(1e10)\n",
    "    X_all = X_all_unrestricted[:actual_restrict_size]\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    \n",
    "    for slice_i in range(math.ceil(data_len / batch_size)):\n",
    "        idx = slice_i * batch_size\n",
    "        #X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]*255  # bugfix: thanks Zezhou Sun!\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch.astype(np.uint8), y_batch.astype(np.uint8)\n",
    "\n",
    "cifar10_dataset_generators = {\n",
    "    'train': cifar10_dataset_generator('train', 1000),\n",
    "    'test': cifar10_dataset_generator('test', -1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load cifar-10 data\n",
    "cf10_tr=cf10.load_training_data()\n",
    "cf10_tr_img=cf10_tr[0]\n",
    "cf10_tr_label = cf10_tr[1]\n",
    "\n",
    "cf10_test=cf10.load_test_data()\n",
    "cf10_test_img=cf10_test[0]\n",
    "cf10_test_label = cf10_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf10_test_img_gray=(cf10_test_img[:,:,:,0]+cf10_test_img[:,:,:,1]+cf10_test_img[:,:,:,2])/3.\n",
    "cf10_tr_img_gray=(cf10_tr_img[:,:,:,0]+cf10_tr_img[:,:,:,1]+cf10_tr_img[:,:,:,2])/3.\n",
    "cf10_tr_vec=np.zeros((50000,1024))\n",
    "cf10_test_vec=np.zeros((10000,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "def img2block(im):\n",
    "    '''\n",
    "    Image patching code. It patches a given RGB image into 32x32 blocks and returns a 4D array with size \n",
    "    [number_of_patches,32,32,3]\n",
    "    '''\n",
    "    im = im.astype(np.float32)\n",
    "    row,col,color = im.shape\n",
    "    im_bl=np.zeros((int(row*col/1024),32,32,3)).astype(np.float32)\n",
    "    count=0\n",
    "    for i in range(0,row-row%32,32):\n",
    "        for j in range(0,col-col%32,32):\n",
    "            im_bl[count,:,:,:]=im[i:i+32,j:j+32,:]\n",
    "            count = count +1\n",
    "    im_bl=im_bl/255.\n",
    "    return im_bl\n",
    "\n",
    "def block2img(img_blocks,img_size):\n",
    "    '''\n",
    "    Function for reconstructing the image back from patches\n",
    "    '''\n",
    "    row,col = img_size\n",
    "    img=np.zeros((row,col,3)).astype(np.float32)\n",
    "    n,k,l,c=img_blocks.shape\n",
    "                 \n",
    "    for i in range(0,int(row/k)):\n",
    "        for j in range(0,int(col/k)):\n",
    "            img[i*k:(i+1)*k,j*l:(j+1)*l,:]=img_blocks[int(i*col/k+j),:,:,:]\n",
    "    return img\n",
    "\n",
    "#Get the patches of lena image\n",
    "lena_img = skimage.io.imread('../test_img/lena512color.tiff')\n",
    "lena_32=img2block(lena_img)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert2uint8(img):\n",
    "    img[img>255]=255\n",
    "    img[img<0]=0\n",
    "    return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1 - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tr = cf10_tr_vec.astype(np.float32)/255.\n",
    "x_test = cf10_test_vec.astype(np.float32)/255.\n",
    "x_test=x_test[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlp1(x, hidden_sizes, activation_fn=tf.nn.relu,dropout_rate=1.0,std_dev=1.0):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=std_dev)}\n",
    "    for k in range(len(hidden_sizes)-1):\n",
    "        layer_name=\"weights\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[k]])\n",
    "            b = tf.get_variable('b', shape=[hidden_sizes[k]])\n",
    "            x = activation_fn(tf.matmul(x, W) + b)\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[-1]])\n",
    "        b = tf.get_variable('b', shape=[hidden_sizes[-1]])\n",
    "        return tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def mlp2(x, hidden_sizes_1,hidden_sizes_2, activation_fn=tf.nn.relu,dropout_rate=1.0,std_dev=1.0,cons_mult=1):\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=std_dev)}\n",
    "    for k in range(len(hidden_sizes_1)-1):\n",
    "        layer_name=\"weights_enc\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes_1[k]])\n",
    "            b = tf.get_variable('b', shape=[hidden_sizes_1[k]])\n",
    "            x = activation_fn(tf.matmul(x, W) + cons_mult*b)\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer_enc', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes_1[-1]])\n",
    "        b = tf.get_variable('b', shape=[hidden_sizes_1[-1]])\n",
    "        x = activation_fn(tf.matmul(x, W) + cons_mult*b)\n",
    "        \n",
    "    x_quant = tf.round(x*255.)/255.\n",
    "    \n",
    "    for k in range(len(hidden_sizes_2)-1):\n",
    "        layer_name=\"weights_dec\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes_2[k]])\n",
    "            b = tf.get_variable('b', shape=[hidden_sizes_2[k]])\n",
    "            x = activation_fn(tf.matmul(x, W) + cons_mult*b)\n",
    "            x_quant=(activation_fn(tf.matmul(x_quant, W) + cons_mult*b))\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer_dec', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes_2[-1]])\n",
    "        b = tf.get_variable('b', shape=[hidden_sizes_2[-1]])\n",
    "    \n",
    "    return (tf.matmul(x, W) + cons_mult*b,tf.matmul(x_quant, W) + cons_mult*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4cbc66f5362e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m test_classification(lambda x: mlp2(x, [850,700,500],[700,850,1024],\n\u001b[0;32m---> 50\u001b[0;31m                                   activation_fn=tf.nn.relu,std_dev=1,cons_mult=0.5), learning_rate=1e-3)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-4cbc66f5362e>\u001b[0m in \u001b[0;36mtest_classification\u001b[0;34m(model_function, learning_rate)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mbatch_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# test trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_step' is not defined"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, 1024])\n",
    "            #y_logits = model_function(x_)\n",
    "            y_logits,x_recon = model_function(x_)\n",
    "\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.subtract(x_,y_logits)**2)\n",
    "            loss2=tf.reduce_mean(tf.subtract(x_,x_recon)**2)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "            \n",
    "            #y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            #correct_prediction = tf.equal(y_pred, tf.argmax(y_, 1))\n",
    "            #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        ids=[i for i in range(100)]\n",
    "        for iter_i in range(100001):\n",
    "            batch_xs = x_tr[ids,:] \n",
    "            ids=[(ids[0]+100+i)%x_tr.shape[0] for i in range(100)]\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 1000 == 0:\n",
    "                tf_feed_dict = {x_: batch_xs}\n",
    "                loss_val = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                print('iteration %d\\t train mse: %.3f\\t'%(iter_i,loss_val))\n",
    "                if iter_i%5000 == 0:\n",
    "                    \n",
    "                    loss_val_test = sess.run(loss, feed_dict={x_:x_test})\n",
    "                    loss_val2_test = sess.run(loss2, feed_dict={x_:x_test})\n",
    "                    print('iteration %d\\t TEST MSE: %.3f\\t %.3f\\t'%(iter_i,loss_val_test,loss_val2_test))\n",
    "                \n",
    "                    \n",
    "                    x_from_tr=sess.run(y_logits, feed_dict={x_:batch_xs[:5,:].reshape([-1,1024])})\n",
    "                    x_from_test=sess.run(y_logits, feed_dict={x_:x_test[0:5,:].reshape([-1,1024])})\n",
    "                    \n",
    "                    \n",
    "                \n",
    "#test_classification(lambda x: mlp1(x, [850,700,500,700,850,1024],\n",
    "#                                  activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=1e-3)\n",
    "\n",
    "test_classification(lambda x: mlp2(x, [850,700,500],[700,850,1024],\n",
    "                                  activation_fn=tf.nn.relu,std_dev=1,cons_mult=0.5), learning_rate=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
