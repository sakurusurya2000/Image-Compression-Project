{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read CIFAR images\n",
    "\n",
    "import read_cifar10 as cf10\n",
    "\n",
    "#@read_data.restartable\n",
    "def cifar10_dataset_generator(dataset_name, batch_size, restrict_size=1000):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    X_all_unrestricted, y_all = (cf10.load_training_data() if dataset_name == 'train'\n",
    "                                 else cf10.load_test_data())\n",
    "    \n",
    "    actual_restrict_size = restrict_size if dataset_name == 'train' else int(1e10)\n",
    "    X_all = X_all_unrestricted[:actual_restrict_size]\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    \n",
    "    for slice_i in range(math.ceil(data_len / batch_size)):\n",
    "        idx = slice_i * batch_size\n",
    "        #X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]*255  # bugfix: thanks Zezhou Sun!\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch.astype(np.uint8), y_batch.astype(np.uint8)\n",
    "\n",
    "cifar10_dataset_generators = {\n",
    "    'train': cifar10_dataset_generator('train', 1000),\n",
    "    'test': cifar10_dataset_generator('test', -1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load cifar-10 data\n",
    "cf10_tr=cf10.load_training_data()\n",
    "cf10_tr_img=cf10_tr[0]\n",
    "cf10_tr_label = cf10_tr[1]\n",
    "\n",
    "cf10_test=cf10.load_test_data()\n",
    "cf10_test_img=cf10_test[0]\n",
    "cf10_test_label = cf10_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import skimage.io\n",
    "def img2block(im):\n",
    "    '''\n",
    "    Image patching code. It patches a given RGB image into 32x32 blocks and returns a 4D array with size \n",
    "    [number_of_patches,32,32,3]\n",
    "    '''\n",
    "    im = im.astype(np.float32)\n",
    "    row,col,color = im.shape\n",
    "    im_bl=np.zeros((int(row*col/1024),32,32,3)).astype(np.float32)\n",
    "    count=0\n",
    "    for i in range(0,row-row%32,32):\n",
    "        for j in range(0,col-col%32,32):\n",
    "            im_bl[count,:,:,:]=im[i:i+32,j:j+32,:]\n",
    "            count = count +1\n",
    "    im_bl=im_bl/255.\n",
    "    return im_bl\n",
    "\n",
    "def block2img(img_blocks,img_size):\n",
    "    '''\n",
    "    Function for reconstructing the image back from patches\n",
    "    '''\n",
    "    row,col = img_size\n",
    "    img=np.zeros((row,col,3)).astype(np.float32)\n",
    "    n,k,l,c=img_blocks.shape\n",
    "                 \n",
    "    for i in range(0,int(row/k)):\n",
    "        for j in range(0,int(col/k)):\n",
    "            img[i*k:(i+1)*k,j*l:(j+1)*l,:]=img_blocks[int(i*col/k+j),:,:,:]\n",
    "    return img\n",
    "\n",
    "#Get the patches of lena image\n",
    "lena_img = skimage.io.imread('../test_img/lena512color.tiff')\n",
    "lena_32=img2block(lena_img)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for converting a double to uint8\n",
    "def convert2uint8(img):\n",
    "    img[img>255]=255\n",
    "    img[img<0]=0\n",
    "    return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2 - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create the inputs in the desired format\n",
    "x_tr = cf10_tr_img.astype(np.float32)#*255.\n",
    "x_test = cf10_test_img.astype(np.float32)#*255.\n",
    "x_test=x_test[:200,:,:,:]\n",
    "img = skimage.io.imread('../test_img/lena512color.tiff')\n",
    "img_32=img2block(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_autoencoder(x_,kernels1=[5,7],kernels2=[7,5],filters1=[16,128],filters2=[128,3],pool_size=[1,2,2,1]):\n",
    "    '''\n",
    "    Autoencoder network\n",
    "    \n",
    "    Inputs:\n",
    "    x_ (tf.placeholder) : Input tensor\n",
    "    kernels1 (1D array) : Size of the encoder kernels (assumed square kernels)\n",
    "    kernels2 (1D array) : Size of the decoder kernels (assumed square kernels)\n",
    "    filters1 (1D array) : Number of filters in encoder layers\n",
    "    filters2 (1D array) : Number of filters in decoder layers\n",
    "    pool_size (1D array): Pooling size in each layer. Its length must be equal to len(kernels1)+len(kernels2)\n",
    "                          First len(kernels1) terms will be used as pooling layers of encoder/\n",
    "                          Remainin terms will be used as unpooling layers of decoder\n",
    "                          \n",
    "    Returns:\n",
    "    out_ (tf.placeholder)     : Output of the autoencoder without quantization in the middle\n",
    "    out_quant (tf.placeholder): Output of the autoencoder with quantization in the middle\n",
    "    '''\n",
    "    out_=x_\n",
    "    \n",
    "    #Encoder\n",
    "    for k in range(len(kernels1)):\n",
    "        conv = tf.layers.conv2d(inputs=out_,\n",
    "                                filters=filters1[k],\n",
    "                                kernel_size=[kernels1[k],kernels1[k]],\n",
    "                                padding=\"same\",\n",
    "                                activation=tf.nn.relu,\n",
    "                                name='conv'+str(k))\n",
    "        pool_now=pool_size[k]\n",
    "        if(pool_now==1):\n",
    "            out_=conv\n",
    "        else:\n",
    "            out_ = tf.layers.max_pooling2d(inputs=conv, \n",
    "                                           pool_size=[pool_now,pool_now], \n",
    "                                           strides=pool_now,\n",
    "                                           name = 'pool'+str(k))\n",
    "        \n",
    "        \n",
    "    #Quantization of output\n",
    "    out_quant=tf.round(out_*256.)/256.\n",
    "\n",
    "    #Decoder\n",
    "    for k in range(len(kernels2)):\n",
    "        with tf.variable_scope(\"deconv\") as var_scope:\n",
    "            pool_now=pool_size[k+len(kernels1)]\n",
    "            if(pool_now==1):\n",
    "                x_up=out_\n",
    "                out_ = tf.layers.conv2d(inputs=x_up,\n",
    "                                        filters=filters2[k],\n",
    "                                        kernel_size=[kernels2[k],kernels2[k]],\n",
    "                                        padding=\"same\",\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        name='deconv'+str(k))\n",
    "                var_scope.reuse_variables() \n",
    "                x_quant_up=out_quant\n",
    "                out_quant = tf.layers.conv2d(inputs=x_quant_up,\n",
    "                                            filters=filters2[k],\n",
    "                                            kernel_size=[kernels2[k],kernels2[k]],\n",
    "                                            padding=\"same\",\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            name='deconv'+str(k))\n",
    "            else:\n",
    "                #Bilinear interpolation of images\n",
    "                sh = out_.get_shape().as_list()\n",
    "                x_up=tf.image.resize_images(out_,[sh[1]*pool_now,sh[2]*pool_now])\n",
    "                #Convolution\n",
    "                out_ = tf.layers.conv2d(inputs=x_up,\n",
    "                                        filters=filters2[k],\n",
    "                                        kernel_size=[kernels2[k],kernels2[k]],\n",
    "                                        padding=\"same\",\n",
    "                                        activation=tf.nn.relu,\n",
    "                                        name='deconv'+str(k))\n",
    "                var_scope.reuse_variables() \n",
    "                x_quant_up=tf.image.resize_images(out_quant,[sh[1]*pool_now,sh[2]*pool_now])\n",
    "                out_quant = tf.layers.conv2d(inputs=x_quant_up,\n",
    "                                            filters=filters2[k],\n",
    "                                            kernel_size=[kernels2[k],kernels2[k]],\n",
    "                                            padding=\"same\",\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            name='deconv'+str(k))\n",
    "\n",
    "\n",
    "    return out_,out_quant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_classification_loss_mse(kernels1=[5,7],kernels2=[7,5],\n",
    "                                 filters1=[16,128],filters2=[128,3],\n",
    "                                pool_size=[1,2,2,1],learning_rate=1.,FT=False):\n",
    "    '''\n",
    "    MSE based autoencoder optimizer.\n",
    "    \n",
    "    Inputs:\n",
    "    kernels1 (1D array) : Size of the encoder kernels (assumed square kernels)\n",
    "    kernels2 (1D array) : Size of the decoder kernels (assumed square kernels)\n",
    "    filters1 (1D array) : Number of filters in encoder layers\n",
    "    filters2 (1D array) : Number of filters in decoder layers\n",
    "    pool_size (1D array): Pooling size in each layer. Its length must be equal to len(kernels1)+len(kernels2)\n",
    "                          First len(kernels1) terms will be used as pooling layers of encoder/\n",
    "                          Remainin terms will be used as unpooling layers of decoder\n",
    "    learning_rate(float): Learning rate of the optimizer\n",
    "    FT (boolean)        : Boolean value for fine-tuning operation on decoder weights\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    model_dict          : Dictionary of the required output files\n",
    "    '''\n",
    "    \n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/gpu:0\"):  # use gpu:0 if on GPU\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            (x_out,x_out_quant)=cnn_autoencoder(x_,pool_size=pool_size,kernels1=kernels1,filters1=filters1,\n",
    "                                kernels2=kernels2,filters2=filters2)\n",
    "\n",
    "            mse_loss1=tf.reduce_mean(tf.subtract(x_,x_out)**2)\n",
    "            mse_loss2=tf.reduce_mean(tf.subtract(x_,x_out_quant)**2)\n",
    "            \n",
    "            trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            if(FT):\n",
    "                with tf.variable_scope('deconv', reuse=True) as vs:\n",
    "                    var_list=[v for v in tf.global_variables() if v.name.startswith(vs.name)]\n",
    "                train_op = trainer.minimize(mse_loss1,var_list=var_list)\n",
    "            else:\n",
    "                train_op = trainer.minimize(mse_loss1)\n",
    "\n",
    "    model_dict = {'graph': g, 'inputs': x_,'outputs':x_out, 'train_op': train_op, 'loss1': mse_loss1,'loss2': mse_loss2}\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(model_dict,x_32=img_32, train_every=100, test_every=200, load=False,\n",
    "                learning_rate=1.,fname='cifar10_recon',outname='/tmp/cnn_autoencoder',ftname='/tmp/cnn_autoencoder'):\n",
    "    '''\n",
    "    Inputs:\n",
    "    model_dict: Output of apply_classification_loss_mse\n",
    "    x_tr      : Training images\n",
    "    x_test    : Test Images\n",
    "    x_32      : 32x32 patches of a big image\n",
    "    load      : Boolean for loading the weights from pre-trained network\n",
    "    fname     : Directory to save outputs\n",
    "    outname   : Directory to save (load=False) or load (load=True) weights\n",
    "    ftname    : Directory to save new weights when load+True\n",
    "    '''\n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver=tf.train.Saver()\n",
    "        if(load):\n",
    "            saver.restore(sess, outname)\n",
    "            print(\"Model loaded\")\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ids=[i for i in range(100)]\n",
    "        for iter_i in range(20001):\n",
    "            batch_xs = x_tr[ids,:,:,:] \n",
    "            ids=[(ids[0]+100+i)%x_tr.shape[0] for i in range(100)]\n",
    "            sess.run(model_dict['train_op'], feed_dict={model_dict['inputs']: batch_xs})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % train_every == 0:\n",
    "                tf_feed_dict = {model_dict['inputs']: batch_xs}\n",
    "                loss_val = sess.run(model_dict['loss1'], feed_dict={model_dict['inputs']: batch_xs})\n",
    "                print('iteration %d\\t train mse: %.3E\\t'%(iter_i,loss_val))\n",
    "                if iter_i % test_every == 0:\n",
    "                    #tf_feed_dict = {x_: x_test}\n",
    "                    loss_val1 = sess.run(model_dict['loss1'], feed_dict={model_dict['inputs']: x_test})\n",
    "                    loss_val2 = sess.run(model_dict['loss2'], feed_dict={model_dict['inputs']: x_test})\n",
    "                    print('iteration %d\\t TEST MSE: %.3E\\t TEST MSE(Quantized): %.3E\\t'%(iter_i,loss_val1,loss_val2))\n",
    "                    \n",
    "                    img_block=sess.run(model_dict['outputs'], \n",
    "                                       feed_dict={model_dict['inputs']:img_32})\n",
    "                    x_from_test=sess.run(model_dict['outputs'], \n",
    "                                         feed_dict={model_dict['inputs']:x_test[:5,:,:,:].reshape([-1,32,32,3])})\n",
    "                    \n",
    "                    img_recon=block2img(img_block,(512,512))\n",
    "                    img_recon = convert2uint8(img_recon*255.)\n",
    "                    skimage.io.imsave('../'+fname+'/img32_recon_'+str(int(iter_i/test_every))+'.tiff',img_recon)\n",
    "\n",
    "                    for i in range(5):\n",
    "                        img_recon=convert2uint8((255*x_from_test[i,:,:,:]).reshape([32,32,3])).astype(np.uint8)\n",
    "                        skimage.io.imsave('../'+fname+'/test'+str(i)+'_'+str(int(iter_i/test_every))+'.tiff',img_recon)\n",
    "                        \n",
    "        saver = tf.train.Saver()\n",
    "        if(load):\n",
    "            outname=ftname\n",
    "        save_path = saver.save(sess, outname)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\t train mse: 3.307E-01\t\n",
      "iteration 0\t TEST MSE: 2.928E-01\t TEST MSE(Quantized): 2.928E-01\t\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[11616,32,32,128]\n\t [[Node: conv0/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_21, conv0/kernel/read)]]\n\t [[Node: deconv_3/deconv3/Relu/_23 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_84_deconv_3/deconv3/Relu\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'conv0/convolution', defined at:\n  File \"/home/mtezcan/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-22-a5a5b798fb30>\", line 5, in <module>\n    pool_size=[1,2,2,1,1,2,2,1],learning_rate=7e-5)\n  File \"<ipython-input-8-2fc4f18c75a0>\", line 27, in apply_classification_loss_mse\n    kernels2=kernels2,filters2=filters2)\n  File \"<ipython-input-7-e28e59e657f3>\", line 28, in cnn_autoencoder\n    name='conv'+str(k))\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 511, in conv2d\n    return layer.apply(inputs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 290, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 156, in call\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 661, in convolution\n    op=op)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 331, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 653, in op\n    name=name)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 129, in _non_atrous_convolution\n    name=name)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 403, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[11616,32,32,128]\n\t [[Node: conv0/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_21, conv0/kernel/read)]]\n\t [[Node: deconv_3/deconv3/Relu/_23 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_84_deconv_3/deconv3/Relu\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[11616,32,32,128]\n\t [[Node: conv0/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_21, conv0/kernel/read)]]\n\t [[Node: deconv_3/deconv3/Relu/_23 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_84_deconv_3/deconv3/Relu\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a5a5b798fb30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                      pool_size=[1,2,2,1,1,2,2,1],learning_rate=7e-5)\n\u001b[1;32m      6\u001b[0m saver = train_model(model_dict, [], train_every=100, test_every=2000,load=False,\n\u001b[0;32m----> 7\u001b[0;31m                     fname='cifar10_recon0',outname='/tmp/cnnx4_test0')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-c5f376416243>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_dict, x_32, train_every, test_every, load, learning_rate, fname, outname, ftname)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     img_block=sess.run(model_dict['outputs'], \n\u001b[0;32m---> 41\u001b[0;31m                                        feed_dict={model_dict['inputs']:img_32})\n\u001b[0m\u001b[1;32m     42\u001b[0m                     x_from_test=sess.run(model_dict['outputs'], \n\u001b[1;32m     43\u001b[0m                                          feed_dict={model_dict['inputs']:x_test[:5,:,:,:].reshape([-1,32,32,3])})\n",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[11616,32,32,128]\n\t [[Node: conv0/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_21, conv0/kernel/read)]]\n\t [[Node: deconv_3/deconv3/Relu/_23 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_84_deconv_3/deconv3/Relu\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'conv0/convolution', defined at:\n  File \"/home/mtezcan/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-22-a5a5b798fb30>\", line 5, in <module>\n    pool_size=[1,2,2,1,1,2,2,1],learning_rate=7e-5)\n  File \"<ipython-input-8-2fc4f18c75a0>\", line 27, in apply_classification_loss_mse\n    kernels2=kernels2,filters2=filters2)\n  File \"<ipython-input-7-e28e59e657f3>\", line 28, in cnn_autoencoder\n    name='conv'+str(k))\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 511, in conv2d\n    return layer.apply(inputs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 290, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 156, in call\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 661, in convolution\n    op=op)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 331, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 653, in op\n    name=name)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 129, in _non_atrous_convolution\n    name=name)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 403, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/mtezcan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[11616,32,32,128]\n\t [[Node: conv0/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_21, conv0/kernel/read)]]\n\t [[Node: deconv_3/deconv3/Relu/_23 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_84_deconv_3/deconv3/Relu\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "#YOU NEED TO CREATE A FOLDER NAMED 'cifar10_recon0' BEFORE RUNNING THAT CODE\n",
    "tf.reset_default_graph()\n",
    "model_dict=apply_classification_loss_mse(kernels1=[5,7,9,9],kernels2=[9,7,7,5],\n",
    "                                     filters1=[128,64,16,4],filters2=[8,8,3,3],\n",
    "                                     pool_size=[1,2,2,1,1,2,2,1],learning_rate=7e-5)\n",
    "saver = train_model(model_dict, [], train_every=100, test_every=2000,load=False,\n",
    "                    fname='cifar10_recon0',outname='/tmp/cnnx4_test0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/cnnx4_test0\n",
      "Model loaded\n",
      "iteration 0\t train mse: 1.189E-02\t\n",
      "iteration 0\t TEST MSE: 3.846E-02\t TEST MSE(Quantized): 3.847E-02\t\n",
      "iteration 100\t train mse: 1.199E-02\t\n",
      "iteration 200\t train mse: 1.219E-02\t\n",
      "iteration 300\t train mse: 1.198E-02\t\n",
      "iteration 400\t train mse: 1.204E-02\t\n",
      "iteration 500\t train mse: 1.206E-02\t\n",
      "iteration 600\t train mse: 1.215E-02\t\n",
      "iteration 700\t train mse: 1.214E-02\t\n",
      "iteration 800\t train mse: 1.233E-02\t\n",
      "iteration 900\t train mse: 1.271E-02\t\n",
      "iteration 1000\t train mse: 1.283E-02\t\n",
      "iteration 1000\t TEST MSE: 3.834E-02\t TEST MSE(Quantized): 3.834E-02\t\n",
      "Model saved in file: /tmp/cnnx4_test0_lion\n"
     ]
    }
   ],
   "source": [
    "#Code for fine-tuning\n",
    "\n",
    "img = skimage.io.imread('../test_img/lena512color.tiff')\n",
    "img_32=img2block(img)\n",
    "x_tr=img_32.copy()\n",
    "\n",
    "tmp=x_tr.copy()\n",
    "x_tr=img_32.copy()\n",
    "idx=np.random.permutation(x_tr.shape[0])\n",
    "x_tr=x_tr[idx,:,:,:]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model_dict=apply_classification_loss_mse(kernels1=[5,7,9],kernels2=[9,7,5],\n",
    "                                     filters1=[64,16,3],filters2=[3,3,3],\n",
    "                                     pool_size=[1,2,2,2,2,1],learning_rate=1e-6,FT=True)\n",
    "saver = train_model(model_dict, [], train_every=100, test_every=1000,load=True,\n",
    "                    fname='cifar10_recon0',outname='/tmp/cnnx4_test0',ftname='/tmp/cnnx4_test0_lion')\n",
    "\n",
    "x_tr=tmp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/cnnx4_test0_lion\n",
      "Model loaded\n",
      "5\n",
      "1th slice\n",
      "2th slice\n",
      "3th slice\n",
      "4th slice\n",
      "5th slice\n"
     ]
    }
   ],
   "source": [
    "#Apply the pre-trained netowrk on another image\n",
    "\n",
    "tfsave ='/tmp/cnnx4_test0_lion'\n",
    "imgpath = '../test_img/lion.tiff'\n",
    "outpath='../test_img/lion_recon2_convrealFTpx8.tiff'\n",
    "with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "#with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, tfsave)\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "    img = skimage.io.imread(imgpath)\n",
    "    [w,l,c]=img.shape\n",
    "    img_32=img2block(img)\n",
    "    img_block = np.zeros(img_32.shape)\n",
    "    \n",
    "    n= np.floor(img_32.shape[0]/2000).astype(int)\n",
    "    print(n)\n",
    "    for i in range(0,n):\n",
    "        print(str(i+1)+'th slice')\n",
    "        img_block[i*2000:(i+1)*2000,:,:,:]=sess.run(model_dict['outputs'], \n",
    "                                    feed_dict={model_dict['inputs']:img_32[i*2000:(i+1)*2000,:,:,:]})\n",
    "    img_block[n*2000:,:,:,:]=sess.run(model_dict['outputs'], \n",
    "                                    feed_dict={model_dict['inputs']:img_32[n*2000:,:,:,:]})\n",
    "        \n",
    "    img_recon=block2img(img_block,(w,l))\n",
    "    img_recon = convert2uint8(img_recon*255.)\n",
    "    skimage.io.imsave(outpath,img_recon)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
