{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import read_cifar10 as cf10\n",
    "\n",
    "#@read_data.restartable\n",
    "def cifar10_dataset_generator(dataset_name, batch_size, restrict_size=1000):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    X_all_unrestricted, y_all = (cf10.load_training_data() if dataset_name == 'train'\n",
    "                                 else cf10.load_test_data())\n",
    "    \n",
    "    actual_restrict_size = restrict_size if dataset_name == 'train' else int(1e10)\n",
    "    X_all = X_all_unrestricted[:actual_restrict_size]\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    \n",
    "    for slice_i in range(math.ceil(data_len / batch_size)):\n",
    "        idx = slice_i * batch_size\n",
    "        #X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]*255  # bugfix: thanks Zezhou Sun!\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch.astype(np.uint8), y_batch.astype(np.uint8)\n",
    "\n",
    "cifar10_dataset_generators = {\n",
    "    'train': cifar10_dataset_generator('train', 1000),\n",
    "    'test': cifar10_dataset_generator('test', -1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#Load cifar-10 data\n",
    "\n",
    "cf10_tr=cf10.load_training_data()\n",
    "cf10_tr_img=cf10_tr[0]\n",
    "cf10_tr_label = cf10_tr[1]\n",
    "print(cf10_tr_img.shape)\n",
    "\n",
    "cf10_test=cf10.load_test_data()\n",
    "cf10_test_img=cf10_test[0]\n",
    "cf10_test_label = cf10_test[1]\n",
    "print(cf10_test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf10_test_img_gray=(cf10_test_img[:,:,:,0]+cf10_test_img[:,:,:,1]+cf10_test_img[:,:,:,2])/3.\n",
    "cf10_tr_img_gray=(cf10_tr_img[:,:,:,0]+cf10_tr_img[:,:,:,1]+cf10_tr_img[:,:,:,2])/3.\n",
    "cf10_tr_vec=np.zeros((50000,1024))\n",
    "cf10_test_vec=np.zeros((10000,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test is done\n",
      "train is done\n"
     ]
    }
   ],
   "source": [
    "#Calculate the metrics for jpg\n",
    "\n",
    "\n",
    "mse_jpg=np.zeros((10000,1))\n",
    "psnr_jpg=np.zeros((10000,1))\n",
    "#Save cifar test images\n",
    "for k in range(10000):\n",
    "    img_gray=(255*cf10_test_img_gray[k,:,:]).astype(np.uint8)\n",
    "    skimage.io.imsave('../cifar10_jpg/'+str(k)+'.jpg',img_gray)\n",
    "    \n",
    "    img_vec = img_gray.reshape([1,-1])\n",
    "    cf10_test_vec[k,:]=img_vec\n",
    "    img_recons = skimage.io.imread('../cifar10_jpg/'+str(k)+'.jpg')\n",
    "    mse=float(((img_recons-img_gray)**2).mean())/255.\n",
    "    mse_jpg[k]=mse\n",
    "    psnr_jpg[k]=10.*np.log10(1./mse)\n",
    "\n",
    "print('test is done')\n",
    "for k in range(50000):\n",
    "    img_gray=(255*cf10_tr_img_gray[k,:,:]).astype(np.uint8)\n",
    "    img_vec = img_gray.reshape([1,-1])\n",
    "    cf10_tr_vec[k,:]=img_vec\n",
    "    \n",
    "print('train is done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9995\n",
      "9996\n",
      "9997\n",
      "9998\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "A= np.array([[1,2,3],[2,4,5]])\n",
    "#print(np.array([[1,2,3]]).reshape([-1]).tolist() )\n",
    "\n",
    "for k in range(9995,10000):\n",
    "    if(cf10_test_vec[k,:].reshape([-1]).tolist() in cf10_tr_vec.tolist()):\n",
    "        print(str(k)+':(')\n",
    "    else:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.57181456769e-06\n",
      "10.1233693327\n"
     ]
    }
   ],
   "source": [
    "print(mse_jpg.mean()/(255.*255.))\n",
    "print(psnr_jpg.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1 - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6ba8b3beb45b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf10_test_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "x_tr = cf10_tr_vec.astype(np.float32)/255.\n",
    "x_test = cf10_test_vec.astype(np.float32)/255.\n",
    "x_test=x_test[:1000,:]\n",
    "pprint(x_tr)\n",
    "pprint(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def mlp1(x, hidden_sizes, activation_fn=tf.nn.relu,dropout_rate=1.0,std_dev=1.0):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=std_dev)}\n",
    "    for k in range(len(hidden_sizes)-1):\n",
    "        layer_name=\"weights\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[k]])\n",
    "            b = tf.get_variable('b', shape=[hidden_sizes[k]])\n",
    "            x = activation_fn(tf.matmul(x, W) + b)\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[-1]])\n",
    "        b = tf.get_variable('b', shape=[hidden_sizes[-1]])\n",
    "        return tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def mlp2(x, hidden_sizes_1,hidden_sizes_2, activation_fn=tf.nn.relu,dropout_rate=1.0,std_dev=1.0):\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=std_dev)}\n",
    "    for k in range(len(hidden_sizes_1)-1):\n",
    "        layer_name=\"weights_enc\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes_1[k]])\n",
    "            b = tf.get_variable('b', shape=[hidden_sizes_1[k]])\n",
    "            x = activation_fn(tf.matmul(x, W) + b)\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer_enc', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes_1[-1]])\n",
    "        b = tf.get_variable('b', shape=[hidden_sizes_1[-1]])\n",
    "        x = activation_fn(tf.matmul(x, W) + b)\n",
    "        \n",
    "    x_quant = tf.round(x*255.)/255.\n",
    "    \n",
    "    for k in range(len(hidden_sizes_2)-1):\n",
    "        layer_name=\"weights_dec\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes_2[k]])\n",
    "            b = tf.get_variable('b', shape=[hidden_sizes_2[k]])\n",
    "            x = activation_fn(tf.matmul(x, W) + b)\n",
    "            x_quant=(activation_fn(tf.matmul(x_quant, W) + b))\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer_dec', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes_2[-1]])\n",
    "        b = tf.get_variable('b', shape=[hidden_sizes_2[-1]])\n",
    "    \n",
    "    return (tf.matmul(x, W) + b,tf.matmul(x_quant, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\t train mse: 6.549\t\n",
      "iteration 0\t TEST MSE: 7.306\t 7.306\t\n",
      "iteration 1000\t train mse: 0.025\t\n",
      "iteration 2000\t train mse: 0.022\t\n",
      "iteration 3000\t train mse: 0.021\t\n",
      "iteration 4000\t train mse: 0.021\t\n",
      "iteration 5000\t train mse: 0.021\t\n",
      "iteration 5000\t TEST MSE: 0.021\t 0.021\t\n",
      "iteration 6000\t train mse: 0.021\t\n",
      "iteration 7000\t train mse: 0.021\t\n",
      "iteration 8000\t train mse: 0.021\t\n",
      "iteration 9000\t train mse: 0.021\t\n",
      "iteration 10000\t train mse: 0.021\t\n",
      "iteration 10000\t TEST MSE: 0.021\t 0.021\t\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, 1024])\n",
    "            #y_logits = model_function(x_)\n",
    "            y_logits,x_recon = model_function(x_)\n",
    "\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.subtract(x_,y_logits)**2)\n",
    "            loss2=tf.reduce_mean(tf.subtract(x_,x_recon)**2)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "            \n",
    "            #y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            #correct_prediction = tf.equal(y_pred, tf.argmax(y_, 1))\n",
    "            #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        ids=[i for i in range(100)]\n",
    "        for iter_i in range(1000001):\n",
    "            batch_xs = x_tr[ids,:] \n",
    "            ids=[(ids[0]+100+i)%x_tr.shape[0] for i in range(100)]\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 1000 == 0:\n",
    "                tf_feed_dict = {x_: batch_xs}\n",
    "                loss_val = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                print('iteration %d\\t train mse: %.3f\\t'%(iter_i,loss_val))\n",
    "                if iter_i%5000 == 0:\n",
    "                    \n",
    "                    loss_val_test = sess.run(loss, feed_dict={x_:x_test})\n",
    "                    loss_val2_test = sess.run(loss2, feed_dict={x_:x_test})\n",
    "                    print('iteration %d\\t TEST MSE: %.3f\\t %.3f\\t'%(iter_i,loss_val_test,loss_val2_test))\n",
    "                '''\n",
    "                    x_from_tr=sess.run(y_logits, feed_dict={x_:batch_xs[:5,:].reshape([-1,1024])})\n",
    "                    x_from_test=sess.run(y_logits, feed_dict={x_:x_test[0:5,:].reshape([-1,1024])})\n",
    "                    print('Train')\n",
    "                    print((x_from_tr-batch_xs[:5,:])**2)\n",
    "                    print('Test')\n",
    "                    print((x_from_test-x_test[:5,:])**2)\n",
    "                '''\n",
    "                    \n",
    "                \n",
    "#test_classification(lambda x: mlp1(x, [850,700,500,700,850,1024],\n",
    "#                                  activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=1e-3)\n",
    "\n",
    "test_classification(lambda x: mlp2(x, [1024],[1024],\n",
    "                                  activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
